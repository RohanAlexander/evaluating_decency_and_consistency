---
title: "Evaluating the Decency and Consistency of Data Validation Tests Generated by LLMs"
author: 
    - name: Rohan Alexander^[Information and Statistical Sciences, University of Toronto. rohan.alexander@utoronto.ca.]
    - name: Lindsay Katz^[The Investigative Journalism Foundation]
    - name: Callandra Moore^[The Investigative Journalism Foundation]
    - name: Zane Schwartz^[The Investigative Journalism Foundation]
date: today
date-format: long
abstract: "We investigated the potential of large language models (LLMs) in developing dataset validation tests. We carried out 96 experiments each for both GPT-3.5 and GPT-4, examining different prompt scenarios, learning modes, temperature settings, and roles. The prompt scenarios were: 1) Asking for expectations, 2) Asking for expectations with a given context, 3) Asking for expectations after requesting a simulation, and 4) Asking for expectations with a provided data sample. For learning modes, we tested: 1) zero-shot, 2) one-shot, and 3) few-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and 1. Furthermore, two distinct roles were considered: 1) “helpful assistant”, 2) “expert data scientist”. To gauge consistency, every setup was tested five times. The LLM-generated responses were benchmarked against a gold standard suite, created by an experienced data scientist knowledgeable about the data in question. We find there are considerable returns to the use of few-shot learning, and that the more explicit the data setting can be the better. The results of the best combinations complement, but do not replace, those of the gold standard. The best LLM configurations complement, rather than substitute, the gold standard results. This study underscores the value LLMs can bring to the data cleaning and preparation stages of the data science workflow."
thanks: "We thank Monica Alexander. Code and data: https://github.com/RohanAlexander/evaluating_decency_and_consistency. Contributions: RA had the initial idea. RA and CM developed the experimental set-up. ZS established the political donations datasets. LK developed the initial suite of dataset validation tests that we compare the LLMs against. RA wrote the code to interact with the LLMs, evaluated the LLM outputs, and did the modelling. All authors contributed to writing the initial draft as well as improving and finalizing the paper."
bibliography: "bibliography.bib"
format: pdf
header-includes:
  - "\\usepackage{listings}"
  - "\\usepackage{fvextra}"
number-sections: true
---

# Introduction

The Investigative Journalism Foundation (IJF) created and maintains a dataset related to political donations in Canada. As of September 2023, the dataset comprises 9,204,112 observations and 15 variables. Every day new observations are added, based on newly released donations records made available by provincial and federal elections agencies. This release cycle is variable, with periods of inactivity followed by bursts of multiple releases. @katzandmoore manually construct an extensive suite of automated tests for this dataset. These impose certain minimum standards on the dataset, including: that constituent aspects add to match any total, class is appropriate, and null values are where expected. This suite allows researchers to use the dataset with confidence and ensures that new additions are fit for purpose.

We revisit this suite of tests to determine whether we can use large language models (LLMs) to mimic this suite of validation tests. We consider a variety of prompts, roles, learning modes, and temperature settings, resulting in 96 total experiments. In particular, we consider four prompt variations: ask for expectations; ask for expectations given described context; ask for expectations having first asked for a simulation; and ask for expectations given a sample of data. We also consider zero-, one-, and few-shot learning; four temperature values: 0, 0.4, 0.6, and 1; and two roles: helpful assistant and expert data scientist. For every combination we obtain five responses from the LLM. We run all experiments separately for both GPT-3.5 and GPT-4.

A human coder judges these responses produced by the LLMs, rating their decency (1-5, where 1 is the worst and 5 is the best) and their consistency (1-5, where 1 means they are very different and 5 means they are essentially identical). We then build an ordinal regression model with `rstanarm` to explore the relationships that decency and consistency have with prompts, roles, learning modes, and temperature settings.

We find there are considerable returns to one- and few-shot learning. We also find that including detailed descriptions of the expected dataset can help improve the quality of the tests that are produced, even more than including an example of the dataset. Surprisingly, there was not much of a difference found between GPT-3.5 and GPT-4. 

Our results demonstrate one use for LLMs in a data analysis workflow, outside of just analysis. In particular, data scientists are often concerned that results may be an artifact of some errors in the data cleaning and preparation pipeline. Data validation tests help assuage these concerns, and using LLMs to establish an initial suite of tests can encourage their use.

The remainder of this paper is structured as follows: @sec-background provides a brief overview of the underlying dataset about which we are writing tests, as well as LLMs. @sec-data details the human coded LLM responses, which is the analysis dataset used in this paper. @sec-model specifies the analysis model used and @sec-results details the estimates. Finally @sec-discussion discusses some of the implications of this study and details a few weaknesses.

# Background {#sec-background}

## The political donations dataset

The Investigative Journalism Foundation (IJF) is a nonprofit news media outlet that is centered around public interest journalism. Their mandate is to help rebuild trust in Canadian democracy and hold the powerful accountable through data-driven investigative reporting. A core component of the IJF's work is building and actively maintaining eight publicly available databases with information on political donations, registered charities, and political lobbying in Canada. While the information that these databases are built upon are ostensibly public, they are not maintained or available in a way that is widely accessible or conducive to analysis. Further, the format and accessibility of these data vary greatly over time and across jurisdictions, making it incredibly difficult to look at temporal or regional differences in the data. In turn, the IJFs collation of these databases and high-quality journalism informed by these data serves as a crucial contribution to rebuilding trust and transparency in Canadian democracy.

One of the IJFs eight databases, and the focus of this work, is the political donations database. Canadian legislation requires political parties and candidates to disclose records of financial contributions they receive. These records are maintained by elections agencies across provinces and territories, and at the federal level. The frequency and scope of these disclosures vary across jurisdictions. For instance, in British Columbia, parties, candidates, constituency associations, and leadership and nomination contestants can receive political donations, while in Newfoundland and Labrador, donation recipients are limited to only parties and candidates [@ijfmethods]. The IJF's political donations database is a compilation of these political finance records across all Canadian jurisdictions, with data spanning from 1993 to the present day. The database contains 14 variables including the donor's name, the political party and entity to which the donation was made, the amount donated, as well as the region and year of the donation.

While the IJF's database is available in a clean, user-friendly format, the original records upon which it was created were not all accessible in this way. The format of donation records varies across jurisdiction and time. While some are available in readily downloadable spreadsheets, others are available as PDF or HTML files --- the former necessitating the use of optical character recognition (OCR) technology [@ijfmethods]. To prepare their database for publication, the IJF team performed significant manual cleaning. The majority of this work resulted from the conversion of PDF donation records to rectangular CSV format using OCR which is prone to scanning-related errors, such as the number 0 being scanned as the letter O. The IJF manually corrected these errors wherever they were identified by carefully comparing the machine-legible OCR output to the original static PDF donations record [@ijfmethods].

Additional cleaning was done for the purpose of data legibility. For instance, the IJF standardized donation dates to match the YYYY-MM-DD format, and they standardized donor names which were in the format "Surname, First name" to be in the form of "First name Surname" [@ijfmethods]. Party names and donor types were also standardized for consistency, and donation records with an abbreviated party name were supplied with the complete name for that party. Finally, in rows where the donor type was null but only individuals were legally allowed to make donations in that jurisdiction and year, the IJF changed that null entry to be "Individual" [@ijfmethods].

Despite the thoughtful and thorough cleaning performed by the IJF team, the magnitude of these data coupled with their self-reported nature makes them prone to both human and computational error arising from the parsing process. With over 9.2 million rows in this database, it would be a massive undertaking for the IJF to manually identify all errors and inconsistencies present in their data --- whether that is as minor as an incorrectly formatted date, or as major as a reported donation amount thousands of dollars above the legal limit. To address this challenge, the IJF has been exploring the use of computational tools to assess data quality [@katzandmoore]. The team has developed a suite of comprehensive data tests for all eight databases using Python's `Great Expectations` library, which contains a number of pre-defined functions to test that particular characteristics expected of a dataset hold true in the data at large. 

As described by @katzandmoore, the process of developing this test suite was iterative in nature and necessitated significant domain knowledge to develop accurate and valuable tests. For the political donations database, @katzandmoore developed a comprehensive suite of tests pertaining to missingness, formatting, and value expectations in the data. For instance, they set the expectation that for donations made in British Columbia, Ontario, or federally, the `donation_date` entry should not be missing, because those are the only three jurisdictions which collect that variable [@katzandmoore]. They also test that donations data from 2022 align with the 2022 legal donation limits for each jurisdiction, and that, where applicable, the sum of reported monetary and non-monetary contribution amounts add up to the total reported donation amount. @katzandmoore provide complete details on the development and implementation of these tests.

## Large language models

Large language models (LLMs) are customized and refined through in-context learning using prompting [@ouyang2022]. A prompt is a set of natural language instructions which define the parameters and context for the desired output and may include one or more input-output examples. By using this approach, LLMs can apply their existing knowledge gained from training on various datasets to adapt to new contexts specified by the prompt. The outcomes generated by LLMs are highly sensitive to the specific phrasing and structure of these natural language instructions. Consequently, there is ongoing work to establish effective prompt patterns [@white2023].


# Data {#sec-data}

We are interested in the extent to which the LLMs can develop a suite of data validation tests that is similar to a suite developed by an experienced expert data scientist who is familiar with the dataset. To test this, we establish and run a series of experiments where we consider different specifications and then compare the LLM output. In particular, the variables that we consider are:

-   Four prompts:
```
        - The Investigative Journalism Foundation (IJF) created and maintains a CSV dataset related to political donations in Canada. Each observation in the dataset is a donation, and the dataset has the following variables: "index", "amount", "donor_location", "donation_date", "donor_full_name", "donor_type", "political_entity", "political_party", "recipient", "region", "donation_year", "amount_monetary", "amount_non_monetary", "electoral_event", "electoral_district", "added".\n\rPlease write a series of expectations using the Python package great_expectations for this dataset.
        - The Investigative Journalism Foundation (IJF) created and maintains a CSV dataset related to political donations in Canada. Each observation in the dataset is a donation, and the dataset has the following variables: "index", "amount", "donor_location", "donation_date", "donor_full_name", "donor_type", "political_entity", "political_party", "recipient", "region", "donation_year", "amount_monetary", "amount_non_monetary", "electoral_event", "electoral_district", "added". 
            - "amount" is a monetary value that cannot be less than $0. An example observation is "195.46". It is, possible, but unlikely to be more than $1,000.00. It cannot be NA. It should be a numeric. The maximum donation "amount" depends on the value of "region" and "year". For "Federal" is 1675, for "Quebec" is 100 since 2013 and 500 for earlier years, for "British Columbia" is 1309.09, for "Ontario" is 3325, and for "Alberta" is 4300. There is no limit for "Saskatchewan".
            - "amount" should be equal to the sum of "amount_monetary" and "amount_non_monetary".
            - "region" can be one of the following values: "Federal", "Quebec", "British Columbia", "Ontario", "Saskatchewan", "Alberta". It cannot be NA. It should be a factor variable.
            - "donor_full_name" is a string. It cannot be NA. It is usually a first and last name, but might also include a middle initial. It should be in title case.
            - "donation_date" should be a date in the following format: YYYY-MM-DD. It could be NA. The earliest donation is from 2010-01-01. The latest donation is from 2023-09-01.
            - "donation_year" should match the year of "donation_date" if "donation_date" is not NA, but it is possible that "donation_year" exists even if "donation_date" does not. The earliest year is 2010 and the latest year is 2023. This variable is an integer.
            - "political_party" cannot be NA. It should be a factor that is equal to one of: "New Democratic Party", "Liberal Party of Canada", "Conservative Party of Canada".
            
            Please write a series of expectations using the Python package great_expectations for this dataset.
        - The Investigative Journalism Foundation (IJF) created and maintains a CSV dataset related to political donations in Canada. Each observation in the dataset is a donation, and the dataset has the following variables: "index", "amount", "donor_location", "donation_date", "donor_full_name", "donor_type", "political_entity", "political_party", "recipient", "region", "donation_year", "amount_monetary", "amount_non_monetary", "electoral_event", "electoral_district", "added". 
            - "amount" is a monetary value that cannot be less than $0. An example observation is "195.46". It is, possible, but unlikely to be more than $1,000.00. It cannot be NA. It should be a numeric. The maximum donation "amount" depends on the value of "region" and "year". For "Federal" is 1675, for "Quebec" is 100 since 2013 and 500 for earlier years, for "British Columbia" is 1309.09, for "Ontario" is 3325, and for "Alberta" is 4300. There is no limit for "Saskatchewan".
            - "amount" should be equal to the sum of "amount_monetary" and "amount_non_monetary".
            - "region" can be one of the following values: "Federal", "Quebec", "British Columbia", "Ontario", "Saskatchewan", "Alberta". It cannot be NA. It should be a factor variable.
            - "donor_full_name" is a string. It cannot be NA. It is usually a first and last name, but might also include a middle initial. It should be in title case.
            - "donation_date" should be a date in the following format: YYYY-MM-DD. It could be NA. The earliest donation is from 2010-01-01. The latest donation is from 2023-09-01.
            - "donation_year" should match the year of "donation_date" if "donation_date" is not NA, but it is possible that "donation_year" exists even if "donation_date" does not. The earliest year is 2010 and the latest year is 2023. This variable is an integer.
            - "political_party" cannot be NA. It should be a factor that is equal to one of: "New Democratic Party", "Liberal Party of Canada", "Conservative Party of Canada".

            Please simulate an example dataset of 1000 observations. Based on that simulation please write a series of expectations using the Python package great_expectations for this dataset.
        - The Investigative Journalism Foundation (IJF) created and maintains a CSV dataset related to political donations in Canada. Each observation in the dataset is a donation, and the dataset has the following variables: "index", "amount", "donor_location", "donation_date", "donor_full_name", "donor_type", "political_entity", "political_party", "recipient", "region", "donation_year", "amount_monetary", "amount_non_monetary", "electoral_event", "electoral_district", "added". 
            An example of a dataset is: 
            index,amount,donor_location,donation_date,donor_full_name,donor_type,political_entity,political_party,recipient,region,donation_year,amount_monetary,amount_non_monetary,electoral_event,electoral_district,added
            5279105,$20.00,"Granton, N0M1V0",2014-08-15,Shelley Reynolds,Individual,Party,New Democratic Party,New Democratic Party,Federal,2014,20.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00
            2187800,$200.00,,,Robert Toupin,Individual,Party,Coalition Avenir Québec - l'Équipe François Legault,,Quebec,2018,,,,,2023-03-17 18:02:29.706250+00:00
            3165665,$50.00,,,Geneviève Dussault,Individual,Party,Québec Solidaire  (Avant Fusion),,Quebec,2017,,,,,2023-03-19 18:02:24.746621+00:00
            8803473,$250.00,"Nan, Nan",,Roger Anderson,Individual,Party,Reform Party Of Canada,Reform Party Of Canada,Federal,1994,0.0,0.0,Annual,Nan,2022-11-22 02:25:34.868056+00:00
            2000776,"$1,425.00","Calgary, T3H5K2",2018-10-30,Melinda Parker,Individual,Registered associations,Liberal Party Of Canada,Calgary Centre Federal Liberal Association,Federal,2018,1425.0,0.0,Annual,Calgary Centre,2022-11-23 01:00:31.771769+00:00
            9321613,$75.00,,2022-06-17,Jeffrey Andrus,Individual,Party,Bc Ndp,Bc Ndp,British Columbia,2022,,,,,2022-12-21 02:20:49.009276+00:00
            2426288,$50.00,"Stony Plain, T7Z1L5",2018-07-24,Phillip L Poulin,Individual,Party,Conservative Party Of Canada,Conservative Party Of Canada,Federal,2018,50.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00
            4428629,$100.00,"Calgary, T2Y4K1",2015-07-30,Barry Hollowell,Individual,Party,New Democratic Party,New Democratic Party,Federal,2015,100.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00
            1010544,$20.00,"Langley, V1M1P2",2020-05-31,Carole Sundin,Individual,Party,New Democratic Party,New Democratic Party,Federal,2020,20.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00
            4254927,$500.00,"Welshpool, E5E1Z1",2015-10-10,Melville E Young,Individual,Party,Conservative Party Of Canada,Conservative Party Of Canada,Federal,2015,500.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00
            8001740,$90.00,"Deleau, R0M0L0",2004-11-15,Clarke Robson,Individual,Party,New Democratic Party,New Democratic Party,Federal,2004,90.0,0.0,Annual,Nan,2022-11-23 01:00:31.771769+00:00

            Based on this sample please write a series of expectations using the Python package great_expectations for this dataset.
```
-   Zero-, one-, and few-shot learning:
```
        - The following text in quotes is an example of an expectation for this dataset:
            """
            # Check that there is nothing null in any column of donations details
            donations_mv.expect_column_values_to_not_be_null(column='donor_full_name')
            """
        - The following text in quotes is an example of three expectations for this dataset:
            """
            # Check that there is nothing null in any column of donations details
            donations_mv.expect_column_values_to_not_be_null(column='donor_full_name')
            # Check that the federal donation does not exceed the maximum
            donations_mv.expect_column_values_to_be_between(
                column = 'amount',
                max_value = 1675,
                row_condition = 'region=="Federal" & donor_full_name.str.contains("Contributions Of")==False & donor_full_name.str.contains("Estate Of")==False & donor_full_name.str.contains("Total Anonymous Contributions")==False & donation_year == 2022 & political_entity.str.contains("Leadership")==False',
                condition_parser = 'pandas'
            )
            # Check that the date matches an appropriate regex format
            donations_mv.expect_column_values_to_match_regex(column = 'donation_date',
                                                        regex = '\\d{4}-\\d{2}-\\d{2}',
                                                        row_condition = "donation_date.isna()==False",
                                                        condition_parser = 'pandas')
            """
```
-   Four temperature values: 0, 0.4, 0.6, and 1.
-   Two roles:
```
        - You are a helpful assistant.
        - You are a highly-trained, experienced, data scientist who is an expert at writing readable, correct, Python code.
```

This combination of variables and options results in 96 different prompt situations. We run these through both GPT-3.5 and GPT-4, using the API. For every combination we ask for five responses to understand variation.

This results in a dataset of responses. The option that gave rise to each response was blinded and the order randomized, and then the responses were ranked by one experienced human coder on two metrics. The first, "consistency", is a ranking 1-5 of how different each of the five responses was for that particular combination of variables. 1 means that responses 1-5 were wildly different. 5 means that responses 1-5 are entirely or essentially the same. The human coder then ranked the "decency" of the first response for each combination of variables. This is a measure of how effective the LLM validation tests were compared with the code written by the experienced data scientist who wrote the original suite of tests. The LLM responses are not expected to have the full context of the code, so we do not expect an exact match, but it should actually write code, import relevant libraries, add comments, deal with class, and write a variety of relevant expectations. 1 means that the code is unusable, 2 means that it is not unusable but would need a lot of work and would be disappointing from a human, 3 means that it is fine but would need some fixing, 4 means it is broadly equivalent to what the gold standard suite contains, and 5 means it is in no worse and is better in some way than the gold standard validation suite.

```{r}
#| echo: false
#| warning: false

library(tidyverse)

data <- read_csv("../raw_data/responses_with_human_coding.csv")

data <- 
    data |>
    mutate(Prompt_n = factor(Prompt_n, levels = c("Name", "Describe", "Simulate", "Example")),
           Temperature = factor(Temperature),
           Role_n = factor(Role_n, levels = c("Helpful", "Expert")),
           Shot_n = factor(Shot_n, levels = c("Zero", "One", "Few")),
           Version = factor(Version)
    )
```

@fig-version examines how decency and consistency differ based on whether GPT-3.5 or GPT-4 is used. Unexpectedly, GPT-4 has fewer responses rated 5/5 for decency, compared with GPT-3.5 (@fig-version-1). GPT-3.5 has fewer responses rated 2/5, but overall the mean decency response for GPT-3.5 is 3.23, while the mean decency of the responses generated by GPT-4 is 3.01. The consistency is not too different between the two versions, with GPT-3.5 having an average of 3.61, while GPT-4 has an average of 3.65. GPT-4 appears to have fewer responses that are completely identical (@fig-version-2).

```{r}
#| echo: false
#| label: fig-version
#| fig-cap: "How decency and consistency change based on whether GPT-3.5 or GPT-4 is used"
#| fig-subcap: ["Decency","Consistency"]
#| warning: false
#| layout-ncol: 2

data |> 
    ggplot(aes(x = decency, fill = Version)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Decency", y = "Number of occurrences"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")

data |> 
    ggplot(aes(x = consistency, fill = Version)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Consistency", y = "Number of occurrences"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```

@fig-prompt examines how decency and consistency differ based on which prompt is used. The prompts differ by how much information is provided. The least informative prompt, "Name", essentially consists of just providing the LLM with the names of the columns that are expected to be in the dataset. The next most informative prompt, "Describe", adds a detailed description of what we expect of the observations. "Simulate" adds that we expect the LLM to first simulate a dataset based on that description, before generating the expectations. And finally, the most informative prompt, "Example", provides a snapshot of the dataset, consisting of the relevant variables and ten observations.

There appears to be considerable difference in terms of the how the prompts are associated with decency. In particular, "Name" is never associated with a 5/5 rating (@fig-prompt-1). Surprisingly, however, the most informative prompt, "Example", is also never associated with a 5/5 rating. Instead it is "Describe" and "Simulate" that tend to be associated with better decency ratings. This is reflected in the averages, which are 2.65, 3.46, 3.44, and 2.94 for "Name", "Describe", "Simulate", and "Example", respectively.

The pattern is not as clear when it comes to consistency (@fig-prompt-2). All four have similar averages, at 3.71, 3.75, 3.48, and 3.58 for "Name", "Describe", "Simulate", and "Example", respectively. That said, it is clear that a wider variety of responses (as denoted by lower consistency ratings), are rarely seen for "Name" and "Describe".

```{r}
#| echo: false
#| eval: false
#| warning: false

data |>
    group_by(Prompt_n) |>
    summarize(mean_decency = mean(decency),
             mean_consistency = mean(consistency)
                )
```

```{r}
#| echo: false
#| label: fig-prompt
#| fig-cap: "How decency and consistency change based on the type of prompt used"
#| fig-subcap: ["Decency","Consistency"]
#| warning: false
#| layout-ncol: 2

data |> 
    ggplot(aes(x = decency, fill = Prompt_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Decency", y = "Number of occurrences", fill = "Prompt type"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")

data |> 
    ggplot(aes(x = consistency, fill = Prompt_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Consistency", y = "Number of occurrences", fill = "Prompt type"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```

Temperature is a parameter that varies from 0 to 1, that we can use to manipulate how random the LLM is. At high temperatures, the LLM will produce a wider variety of responses. At lower temperatures it will focus on the single most likely response. Higher temperature should be associated with a wider variety of LLM responses.

@fig-temperature examines how decency and consistency differ based on which of the four temperature values we consider---0, 0.4, 0.6, 1---is used. There appears to be limited difference in terms of how different temperature values are associated with decency (@fig-temperature-1). They all have similar mean values at 3.10, 3.25, 2.98, and 3.15 for temperature values of 0, 0.4, 0.6, and 1, respectively.

In contrast, as expected temperature has an effect on consistency. Temperature values of 0 are very clearly associated with ratings of high consistency, and higher temperatures are clearly associated with lower consistency (@fig-temperature-2). Their means differ considerably, with 4.77, 3.75, 3.31, and 2.69 for temperature values of 0, 0.4, 0.6, and 1, respectively.

```{r}
#| echo: false
#| eval: false
#| warning: false

data |>
    group_by(Temperature) |>
    summarize(mean_decency = mean(decency),
             mean_consistency = mean(consistency)
                )
```

```{r}
#| echo: false
#| label: fig-temperature
#| fig-cap: "How decency and consistency change based on temperature"
#| fig-subcap: ["Decency","Consistency"]
#| warning: false
#| layout-ncol: 2

data |> 
    ggplot(aes(x = decency, fill = Temperature)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Decency", y = "Number of occurrences"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")

data |> 
    ggplot(aes(x = consistency, fill = Temperature)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Consistency", y = "Number of occurrences"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```

Role is an aspect of a prompt that is provided to the LLM before the main prompt content. We used two different roles, one that positioned the LLM as a helpful assistant, and the other that positioned the LLM as an experienced data scientist (@fig-role). We were expecting that the expert role would result in better code, but there was no obvious difference in terms of decency (@fig-role-1). There was also no obvious difference between the consistency (@fig-role-2). Their means did not differ by much in either case.

```{r}
#| echo: false
#| eval: false
#| warning: false

data |>
    group_by(Role_n) |>
    summarize(mean_decency = mean(decency),
             mean_consistency = mean(consistency)
                )
```

```{r}
#| echo: false
#| label: fig-role
#| fig-cap: "How decency and consistency change based on whether role is 'Helpful' or 'Expert'"
#| fig-subcap: ["Decency","Consistency"]
#| warning: false
#| layout-ncol: 2
#| 
data |> 
    ggplot(aes(x = decency, fill = Role_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Decency", y = "Number of occurrences", fill = "Role"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")

data |> 
    ggplot(aes(x = consistency, fill = Role_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Consistency", y = "Number of occurrences", fill = "Role"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```

Shots refers to the number of examples provided to the LLM as part of the prompt. Zero-shot means that no examples are provided, while one-shot and few-shot refer to one- and a few- examples being provided, respectively. Although the advantage of LLMs such as GPT-3.5 and GPT-4 is that they typically do well with zero-shot learning, we would expect that they will do better with one-shot and few-shot.

We find substantial differences, especially when moving away from zero-shot learning (@fig-shot). In particular, we see that decency of 1/5 is dominated by zero-shot, while zero-shot is under-represented in 5/5 (@fig-shot-1). This is also reflected in the mean decency which for zero-shot is 2.83, while for one- and few-shot learning is 3.34 and 3.19, respectively.

We see this pattern in consistency as well. For instance, zero-shot learning is over-represented in the least consistent responses, both 1/5 and 2/5 (@fig-shot-2). And the mean level of consistency is lower for zero-shot learning, at 3.45, compared with single- and few-shot, at 3.77 and 3.67, respectively.

```{r}
#| echo: false
#| eval: false
#| warning: false

data |>
    group_by(Shot_n) |>
    summarize(mean_decency = mean(decency),
             mean_consistency = mean(consistency)
                )
```

```{r}
#| echo: false
#| label: fig-shot
#| fig-cap: "How decency and consistency change based on zero-, one-, and few-shot learning"
#| fig-subcap: ["Decency","Consistency"]
#| warning: false
#| layout-ncol: 2

data |> 
    ggplot(aes(x = decency, fill = Shot_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Decency", y = "Number of occurrences", fill = "Shots"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")

data |> 
    ggplot(aes(x = consistency, fill = Shot_n)) +
    geom_histogram(position = "dodge2", binwidth = 1) +
    theme_minimal() +
    labs(
        x = "Consistency", y = "Number of occurrences", fill = "Shots"
    ) +
    scale_fill_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```

# Model {#sec-model}

Consistency and decency, our dependent variables, are ordered, categorical, outcomes. As such we model the rating $Y$, an ordinal outcome with $J = 5$ possible categories, using ordered logistic regression. Such a model works by estimating the probability that the outcome is less than or equal to some specific category, which in this case is the coded rank of that response (i.e. 1, 2, 3, 4, 5). Models for consistency and decency are estimated separately. In latent variable formulation, we assume that the observed category $y$ can be related to an underlying continuous latent variable, $y^*$, through a series of cutpoints: 

$$
y=\left\{\begin{array}{ll}
1 & \text { if } y^*<\zeta_1 \\
2 & \text { if } \zeta_1 \leq y^* \\
\vdots & \\
J & \text { if } \zeta_{I-1}<1
\end{array}\right.
$$

where $\zeta$ is a vector of cutpoints. The latent variable $y^*$ is then assumed to have a logistic distribution and is modeled as a linear function of covariates. In our case, we are interested in exploring the relationships that consistency and decency have with model, prompt, temperature, role, and number of shots:
$$
y^* = \beta_1 \cdot \mbox{version}_i + \beta_2 \cdot \mbox{prompt}_i + \beta_3 \cdot \mbox{temp}_i + \beta_4 \cdot \mbox{role}_i + \beta_5 \cdot \mbox{shot}_i
$$

In terms of our predictors, version is a binary variable as to whether we are using GPT-3.5 or GPT-4. We expect that GPT-4 will do better in terms of both decency and consistency than GPT-3.5. Prompt can be one of four values: "Name", "Describe", "Simulate", and "Example". We expect that "Simulate" and "Example" will be associated with higher decency than "Name" and "Describe". However, we expect the opposite relationship with consistency. This is because we expect that what will increase decency will be the more specific guidance provided by the "Simulate" and "Example" prompts, which should also increase the consistency. Temperature can take one of four values: 0, 0.4, 0.6 and 1. We expect that higher temperature values will be associated with less consistency. However, it is difficult to anticipate how temperature should be related to decency. Role can be one of two values: "Helpful", or "Expert", while shots can be one of three: "zero", "one", or "few". In the case of both role and shots, we expect that the "Expert" role, and one- and few-shot learning will be associated with higher decency, and consistency.

We fit this model, separately, for each of consistency and decency, in a Bayesian framework using the package `rstanarm` [@rstanarm] using the R statistical programming language [@citeR]. This computational process requires priors to be placed on $R^2$, the proportion of variance in the outcome that is attributable to the coefficients in a linear model, and the vector of cutpoints $\zeta$. For the prior on $R^2$, we follow @gelmanhillvehtari2020 [p. 276] and assume the mean is 0.3. For the vector cutpoints $\zeta$, we use an uninformative prior of $\text{Dirichlet}(1)$. @stanpolrvignette provides more information about fitting this type of model using `rstanarm`.

Diagnostics are provided in @sec-diagnostics.

```{r}
#| echo: false
#| message: false
#| warning: false
#| include: false

library(rstanarm)

consistency_model_rstanarm <-
  readRDS(file = "../model/consistency_model_rstanarm.rds")

decency_model_rstanarm <- 
   readRDS(file = "../model/decency_model_rstanarm.rds")
```

# Results {#sec-results}

In total we considered 192 observations, which is 96 for each LLM considered. The estimates from our models are shown in @tbl-modelsummaryestimates and @fig-modelsummaryestimates, which were both produced with `modelsummary` [@modelsummary].


```{r}
#| label: tbl-modelsummaryestimates
#| tbl-cap: "Exploring the relationships that consistency and decency have with model, prompt, temperature, role, and number of shots"
#| echo: false
#| message: false
#| warning: false

modelsummary::modelsummary(
  list(
    "Consistency" = consistency_model_rstanarm,
    "Decency" = decency_model_rstanarm
  ),
  statistic = "mad",
  fmt = 2
)
```

```{r}
#| label: fig-modelsummaryestimates
#| fig-cap: "Exploring the relationships that consistency and decency have with model, prompt, temperature, role, and number of shots"
#| echo: false
#| message: false
#| warning: false

modelsummary::modelplot(list(
    "Consistency" = consistency_model_rstanarm,
    "Decency" = decency_model_rstanarm
  ), conf_level = 0.9) +
  labs(x = "Coefficient estimates and 90 per cent credibility intervals") +
  scale_color_brewer(palette = "Set1")
```

@tbl-modelsummaryestimates shows the coefficient estimate, with the mad statistic (mean absolute deviation) in brackets. In @fig-modelsummaryestimates, the dot represents the coefficient estimate, while the lines are 90 per cent credibility intervals.

The intercepts, 1|2, 2|3, 3|4, and 4|5, shown in @tbl-modelsummaryestimates reflect cutpoints, where a rating goes from one category to the next (e.g. from 1 to 2, etc).

The models do not identify a substantial difference between GPT-3.5 and GPT-4 in terms of consistency, and, surprisingly, even a somewhat negative association in terms of decency.

Our models do not identify much difference between the different prompt types in terms of consistency. However, they do find that "Describe" and "Simulate" are associated with increased decency, as is "Example", although surprisingly to a lesser extent. 

As expected, increases in temperature are strongly associated with less consistency. However, we are unable to identify much of a relationship between temperature and decency.  

The models struggle to find any association between which role is used in terms of either consistency or decency. That is, priming the prompt with either "You are a helpful assistant." or "You are a highly-trained, experienced, data scientist who is an expert at writing readable, correct, Python code." did not seem to change much in terms of the coding of the responses.

Finally, one- and few-shot learning is associated with increased decency and slightly increased consistency over zero-shot prompts. That said, the magnitude of this relationship is not overly large.


# Discussion {#sec-discussion}

## On the importance and challenges of writing validation tests

Data validation is a cornerstone of high quality data science workflows. Broadly, data validation is centered around establishing the credibility of individual data values, the coherence of data with itself, and the consistency of data with relevant external sources [@alexander2023]. Data quality — or a lack thereof — can have fundamental downstream effects on the rest of the scientific process. If the data being supplied to a statistical model is incorrect in some way (e.g., a categorical variable is stored as numeric), this will have significant implications for subsequent results and conclusions [@alexander2023; @hynes17; @breck2019; @zhang20; @gao2016]. A powerful tool to implement data validation in practice is automated testing, which intuits data expectations from format, column name, or other meta information and converts those expectations to code [@alexander2023; @zhang20]. Automated validation promotes trustworthiness and transparency. By developing a suite of tests to validate that particular characteristics of a dataset hold true, data scientists uncover fundamental assumptions that underpin their work and equip users with confidence that those assumptions are met in practice. The value of data validation testing is not limited to particular domains --- it is a fundamental component of a reliable data science workflow, regardless of the data at hand [@alexander2023; @gao2016].

There are a number of challenges that accompany the development and implementation of data validation tests. These challenges are particularly important to consider when thinking about data validation efforts across research domains and levels of data science expertise. 

First, implementing automated tests requires the ability to develop code. There are a number of testing-centered libraries in various programming languages. `Great Expectations` — the Python library employed by @katzandmoore — contains various data testing functions and has a data assistant functionality which programmatically explores data to develop tests based on observed characteristics [@gx23]. `pointblank` is an R package which offers predefined automated data testing functionality [@pointblank] and IBM SPSS statistics can perform data validation checks by identifying unexpected or invalid entries in a dataset [@IBM23]. With a focus on data validation for machine learning (ML), @hynes17 developed a data linter tool that detects possible inconsistencies in the data and suggests appropriate transformations for a specific model type. Further, @breck2019 executed an automated validation ML platform at Google to address concerns about the downstream effects of messy data in model training. While various data testing software have been developed and shared, many of these have limited functionality, and the implementation of a test suite may require significant data processing work and necessarily requires programming knowledge [@alexander2023; @zhang20]. From their work on data validation for the IJF @katzandmoore conclude that data tests should not be compromised just to be compatible with predefined test functions that exist in a software package. In order to develop the most valuable suite of tests, it may be necessary to create new variables in a dataset, merge multiple datasets, or create tests which are outside the scope of existing software frameworks, for example. This presents a challenge for individuals who are not trained in or comfortable with computer programming, but wish to programmatically test their data. 

In addition, the development of a comprehensive, bespoke, and accurate data test suite can rarely be done by one data scientist alone. As advocated by @katzandmoore, domain expertise is fundamental to the creation and development of high-quality data tests. For instance, without incorporating the knowledge that donation dates are only collected in three jurisdictions (British Columbia, Ontario and Federal) into their code, @katzandmoore's test for missingness for that variable would have produced inflated failure rates. This is knowledge that was gained through collaborative efforts with IJF team members who were involved in dataset construction. For an individual working on an unfamiliar dataset, seeking out domain knowledge and gaining a thorough understanding of that data is a non-trivial task in and of itself. 

Finally, the process of designing, developing, and deploying data validation tests is iterative in nature and takes a great deal of time [@katzandmoore]. For researchers or journalists who are eager to publish their findings, for instance, there may not be enough incentive or resources for them to do this time-consuming validation work. This challenge is only amplified if an individual was not involved in the dataset construction process, does not have access to vital domain expertise, or does not have programming experience. 

While data validation is incredibly valuable for the production of transparent and trustworthy research, it comes with many challenges which hinder its accessibility. This work presents an opportunity to advocate for an alternative approach to data validation through the use of LLMs. Though there has been a great deal of work done to develop more accessible programming tools for data validation, these tools do not entirely mitigate the challenges which accompany the process, namely the need for programming knowledge, domain expertise, and a great deal of time. Our findings illustrate the potential for LLMs to rapidly produce usable data validation tests, written in code, with limited information --- a promising application to support the promotion of more trustworthy, transparent data workflows across domains.

## On the use of LLMs in data science

The contributions of these experiments and this paper are twofold. Firstly, this work adds to the emerging area of inquiry of prompt engineering for large language models. In doing so, our prompts may serve as examples – both good and bad – for other scientists hoping to implement LLMs into their research and data workflows. Secondly, this work contributes to the academic study of LLMs for the development of data science methodologies. As these tools become more powerful and the precision in our ability to specify their outputs improves, they will become tools to reduce the time and resource burden of writing software and tests. 

Our work is consistent with previous literature demonstrating that LLM performance on complex, user-defined tasks is sensitive to prompt engineering. Prompting is a brittle process in which minor alterations can lead to large fluctuations in performance [@arora2022; @zhao2021], however with OpenAI’s LLMs in particular, GPT-4 [@giorgi2023] seems less susceptible to this than ChatGPT (GPT-3.5). In general, previous research has demonstrated that when using ChatGPT, prompts should provide context [@white2023; @clavie2023], be specific [@white2023; @yong2023; @openaiprompt], define a persona or role [@white2023; @clavie2023], break complex reasoning into smaller steps [@white2023; @henrickson2023], and provide example responses [@brown2020; @arora2022; @openaiprompt] to improve performance on a variety of tasks, including classifying job types [@clavie2023], generating images for construction defect detection [@yong2023], and generating hermeneutically valuable text [@henrickson2023]. 

These observations are predominantly borne out in our experiments. For instance, the more specified prompt (“Describe”) and the prompt eliciting step-by-step thinking (“Simulate”) both show improvements over the simplest prompt (“Name”). However, improvements made by specificity break down when example data is provided (“Example”). We hypothesize that this is due to ChatGPT and GPT-4’s poor performance on numerical and structured data in comparison to more domain-specific LLMs such as BloombergGPT which has been trained on numeric, structured financial data and utilizes a tokenizer more suited to these data [@wu2023]. We do not see significant changes in performance when two different roles are specified to the LLM, which contradicts previous literature [@white2023; @clavie2023]. Finally, our significant improvements moving from zero- to one-shot and marginal to no improvements from one- to few-shot align with existing literature [@giorgi2023].  Some studies have demonstrated ChatGPT and GPT-4’s difficulties in responding to queries in the desired format during zero-shot structured tasks, including named entity recognition [@hu2023] and multiple choice question [@clavie2023]. @giorgi2023 thus attribute performance improvements in one- and few-shot prompts to the in-context examples’ guidance as to the desired output structure. Upon inspection of the outputs from zero-, one-, and few-shot prompts, we believe that this same effect exists for our prompts.

Our work additionally contributes to the growing body of work using LLMs to automate portions of the data science workflow. LLMs have already been integrated into software tools such as GitHub's Co-Pilot [@githubcopilot] and AutoGPT [@autogpt] and IDEs such as IntelliJ [@krochmalski2014] and VSCode [@vscode] to produce code in a variety of languages. These tools are and will continue to assist data scientists to accelerate, resolve bugs in, document, and optimize their code in all parts of the data science workflow. LLMs have also demonstrated potential to automate other onerous or tedious tasks in data science pipelines. These include generating synthetic text data [@chung2023; @yu2023], improving search-based software testing [@lemieux2023], generating unit tests from natural language in a variety of programming languages and contexts [@lahiri2022; @schafer2023], generating property-based tests from API documentation [@vikram2023large], and conducting exploratory data analysis [@ma2023]. As LLM prompting and fine-tuning methods improve, they will reduce technical barriers to conducting transparent, accurate, reproducible analysis, enabling data scientists to be more productive and put their energy towards analysis. higher-order reasoning, and sophisticated inference. The work described in this paper towards automating data validation using LLMs has the capacity to further this evolution. 

## Limitations

There are a variety of limitations of this study. The most notable is that there was only one coder of the LLMs responses. While this should ensure internal consistency, it is possible that some of the coding, especially that for decency, would have benefited from an additional coder, and their responses could then have been averaged. The study also only considers one, reasonably specific, setting. Expanding our approach to consider a variety of settings could be especially beneficial. 

A limitation of our work on prompt engineering (and indeed of prompt engineering generally) is that we are unable to provide fully reliable explanations for improvements in performance. Though we can make convincing speculations, these are ultimately educated guesses. Previous studies have shown that in addition to being brittle to precise semantics [@arora2022; @zhao2021], prompts show improved performance from additional text which provides no new information [@henrickson2023] such as naming the model (e.g., “You are Frederick, a helpful AI”), emulating the chatbot saying that it understands the instructions, asking it to output the “right conclusion”, and providing positive feedback [@clavie2023]. Despite the opacity of the emergent properties of LLMs, we may be able to develop greater understanding of the impacts of prompting choices by testing these prompts on a greater diversity of datasets from across disciplines and contexts. 

Our current experiments have been run in a very limited context --- namely one Canadian political donations dataset. Testing these prompts in a variety of contexts will provide evidence as to whether our prompting strategies are generalizable across datasets in different formats, contexts, and technical regimes. Prompts can be conceptualized as knowledge transfer methods analogous to software patterns which are intended to provide reusable solutions to common problems [@white2023]. This aligns with the goal of automated data validation to reduce the need for domain expertise and resources dedicated to data validation. Thus, in order to improve the explainability of our methods and to ensure our method is generalizable, we must validate that the prompts that are effective on this dataset show similar performance on other datasets from different contexts and in different formats. 


\newpage

# Appendix

## Model diagnostics {#sec-diagnostics}

### Consistency model

```{r}
#| fig-cap: "Consistency model trace plot"
#| echo: false
#| message: false
#| warning: false
#| fig-height: 7

plot(consistency_model_rstanarm, "trace") +
    theme(legend.position = "bottom")
```

```{r}
#| fig-cap: "Consistency model rhat plot"
#| echo: false
#| message: false
#| warning: false

plot(consistency_model_rstanarm, "rhat") +
    theme(legend.position = "bottom")
```
```{r}
#| fig-cap: "Consistency model loo plot"
#| echo: false
#| message: false
#| warning: false

plot(loo(consistency_model_rstanarm))
```
```{r}
#| fig-cap: "Consistency model posterior predictive check"
#| echo: false
#| message: false
#| warning: false

pp_check(consistency_model_rstanarm) +
  theme_classic() +
  theme(legend.position = "bottom")
```
```{r}
#| fig-cap: "Consistency model posterior vs prior"
#| echo: false
#| message: false
#| warning: false

posterior_vs_prior(consistency_model_rstanarm) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  coord_flip()
```


\newpage

### Decency model

```{r}
#| fig-cap: "Decency model trace plot"
#| echo: false
#| message: false
#| warning: false
#| fig-height: 7

plot(decency_model_rstanarm, "trace") +
    theme(legend.position = "bottom")
```

```{r}
#| fig-cap: "Decency model rhat plot"
#| echo: false
#| message: false
#| warning: false

plot(decency_model_rstanarm, "rhat") +
    theme(legend.position = "bottom")
```
```{r}
#| fig-cap: "Decency model loo plot"
#| echo: false
#| message: false
#| warning: false

plot(loo(decency_model_rstanarm))
```
```{r}
#| fig-cap: "Decency model posterior predictive check"
#| echo: false
#| message: false
#| warning: false

pp_check(decency_model_rstanarm) +
  theme_classic() +
  theme(legend.position = "bottom")
```
```{r}
#| fig-cap: "Decency model posterior vs prior"
#| echo: false
#| message: false
#| warning: false

posterior_vs_prior(decency_model_rstanarm) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  coord_flip()
```


\newpage

# References
